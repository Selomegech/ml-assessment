{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wMutkVOobvrW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "from collections import Counter\n",
        "import json\n",
        "import os\n",
        "import requests\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Setup and Configuration ---\n",
        "print(\"Downloading NLTK data...\")\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "print(\"NLTK downloads complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0JPPlQ-i81_",
        "outputId": "16f533e8-35a0-42a2-e845-fa574c362f01"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading NLTK data...\n",
            "NLTK downloads complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set plot style for better aesthetics\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "# --- 1. Data Loading & Automated Download ---\n",
        "# Define file and URL details\n",
        "dataset_url = \"https://www.kaggle.com/api/v1/datasets/download/rmisra/news-category-dataset\"\n",
        "zip_file_name = \"news-category-dataset.zip\"\n",
        "# The name of the JSON file inside the zip archive might vary. We'll use v3 as specified before.\n",
        "json_file_name = 'News_Category_Dataset_v3.json'\n",
        "\n",
        "# --- Function to download the file ---\n",
        "def download_file(url, filename):\n",
        "    \"\"\"Downloads a file from a URL, showing progress.\"\"\"\n",
        "    print(f\"Downloading {filename} from {url}...\")\n",
        "    # Note: Kaggle may require authentication. This direct download might fail.\n",
        "    # A more robust solution involves using the Kaggle API with an API key.\n",
        "    try:\n",
        "        with requests.get(url, stream=True) as r:\n",
        "            r.raise_for_status()\n",
        "            with open(filename, 'wb') as f:\n",
        "                for chunk in r.iter_content(chunk_size=8192):\n",
        "                    f.write(chunk)\n",
        "        print(\"Download successful.\")\n",
        "        return True\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error downloading the file: {e}\")\n",
        "        print(\"Please try downloading the dataset manually from Kaggle or configure your Kaggle API credentials.\")\n",
        "        return False\n",
        "\n",
        "# --- Main data loading logic ---\n",
        "if not os.path.exists(json_file_name):\n",
        "    print(f\"'{json_file_name}' not found.\")\n",
        "    if not os.path.exists(zip_file_name):\n",
        "        download_file(dataset_url, zip_file_name)\n",
        "\n",
        "    if os.path.exists(zip_file_name):\n",
        "        print(f\"Extracting '{zip_file_name}'...\")\n",
        "        try:\n",
        "            with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:\n",
        "                # Find the correct file within the zip archive\n",
        "                zip_contents = zip_ref.namelist()\n",
        "                target_file = None\n",
        "                for name in zip_contents:\n",
        "                    if 'News_Category_Dataset_v3.json' in name:\n",
        "                         target_file = name\n",
        "                         break\n",
        "\n",
        "                if target_file:\n",
        "                    zip_ref.extract(target_file)\n",
        "                    # Rename to the expected filename for consistency\n",
        "                    os.rename(target_file, json_file_name)\n",
        "                    print(f\"Extracted and renamed to '{json_file_name}'.\")\n",
        "                else:\n",
        "                    print(f\"Could not find '{json_file_name}' inside the zip archive.\")\n",
        "        except zipfile.BadZipFile:\n",
        "            print(f\"Error: '{zip_file_name}' is not a valid zip file or is corrupted.\")\n",
        "\n",
        "\n",
        "# Now, try to load the dataframe\n"
      ],
      "metadata": {
        "id": "gdfAdb-fjBWW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # The data is stored as one JSON object per line\n",
        "    with open(json_file_name, 'r') as f:\n",
        "        data = [json.loads(line) for line in f]\n",
        "    df = pd.DataFrame(data)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "    print(f\"Dataset shape: {df.shape}\")\n",
        "except (FileNotFoundError, NameError):\n",
        "    print(f\"Error: Could not load the dataset.\")\n",
        "    print(\"Creating a dummy dataframe to allow the rest of the script to run.\")\n",
        "    df = pd.DataFrame({\n",
        "        'headline': ['Sample headline 1', 'Sample headline 2'],\n",
        "        'short_description': ['This is a sample description.', 'This is another sample text for processing.'],\n",
        "        'category': ['TECH', 'SPORTS']\n",
        "    })\n",
        "\n",
        "# --- 2. Initial Inspection ---\n",
        "print(\"\\n--- Initial Data Inspection ---\")\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Combine headline and short description for a richer text source\n",
        "df['text'] = df['headline'] + ' ' + df['short_description']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uICo6-IFjOSu",
        "outputId": "4e8e7156-1bf6-423f-fd6f-80b5008cfa3b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully.\n",
            "Dataset shape: (209527, 6)\n",
            "\n",
            "--- Initial Data Inspection ---\n",
            "First 5 rows of the dataset:\n",
            "                                                link  \\\n",
            "0  https://www.huffpost.com/entry/covid-boosters-...   \n",
            "1  https://www.huffpost.com/entry/american-airlin...   \n",
            "2  https://www.huffpost.com/entry/funniest-tweets...   \n",
            "3  https://www.huffpost.com/entry/funniest-parent...   \n",
            "4  https://www.huffpost.com/entry/amy-cooper-lose...   \n",
            "\n",
            "                                            headline   category  \\\n",
            "0  Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS   \n",
            "1  American Airlines Flyer Charged, Banned For Li...  U.S. NEWS   \n",
            "2  23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY   \n",
            "3  The Funniest Tweets From Parents This Week (Se...  PARENTING   \n",
            "4  Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS   \n",
            "\n",
            "                                   short_description               authors  \\\n",
            "0  Health experts said it is too early to predict...  Carla K. Johnson, AP   \n",
            "1  He was subdued by passengers and crew when he ...        Mary Papenfuss   \n",
            "2  \"Until you have a dog you don't understand wha...         Elyse Wanshel   \n",
            "3  \"Accidentally put grown-up toothpaste on my to...      Caroline Bologna   \n",
            "4  Amy Cooper accused investment firm Franklin Te...        Nina Golgowski   \n",
            "\n",
            "         date  \n",
            "0  2022-09-23  \n",
            "1  2022-09-23  \n",
            "2  2022-09-23  \n",
            "3  2022-09-23  \n",
            "4  2022-09-22  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Text Preprocessing ---\n",
        "print(\"\\n--- Preprocessing Text Data ---\")\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "punctuation = set(string.punctuation)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Cleans and preprocesses a single text string.\n",
        "    - Tokenizes\n",
        "    - Converts to lowercase\n",
        "    - Removes stopwords and punctuation\n",
        "    - Lemmatizes tokens\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    lemmatized_tokens = [\n",
        "        lemmatizer.lemmatize(token)\n",
        "        for token in tokens\n",
        "        if token.isalpha() and token not in stop_words and token not in punctuation\n",
        "    ]\n",
        "\n",
        "    return lemmatized_tokens\n",
        "\n",
        "df_processed = df.copy()\n",
        "df_processed['processed_text'] = df_processed['text'].apply(preprocess_text)\n",
        "\n",
        "print(\"Preprocessing complete. A 'processed_text' column has been added.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nr2S70ujUpH",
        "outputId": "32e689fc-64b8-4a47-ba66-0410d4ef09ea"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Preprocessing Text Data ---\n",
            "Preprocessing complete. A 'processed_text' column has been added.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Exploratory Data Analysis (EDA) ---\n",
        "print(\"\\n--- Performing Exploratory Data Analysis ---\")\n",
        "\n",
        "# A. Distribution of Document Lengths\n",
        "df_processed['doc_length'] = df_processed['processed_text'].apply(len)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(df_processed['doc_length'], bins=50, kde=True)\n",
        "plt.title('Distribution of Document Lengths (Number of Words)', fontsize=16)\n",
        "plt.xlabel('Document Length', fontsize=12)\n",
        "plt.ylabel('Frequency', fontsize=12)\n",
        "plt.show()\n",
        "\n",
        "# B. Most Frequent Words\n",
        "all_words = [word for tokens in df_processed['processed_text'] for word in tokens]\n",
        "word_counts = Counter(all_words)\n",
        "most_common_words = word_counts.most_common(20)\n",
        "\n",
        "df_most_common = pd.DataFrame(most_common_words, columns=['Word', 'Frequency'])\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x='Frequency', y='Word', data=df_most_common, palette='viridis')\n",
        "plt.title('Top 20 Most Frequent Words', fontsize=16)\n",
        "plt.xlabel('Frequency', fontsize=12)\n",
        "plt.ylabel('Word', fontsize=12)\n",
        "plt.show()\n",
        "\n",
        "# C. Category Distribution\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.countplot(y='category', data=df_processed, order=df_processed['category'].value_counts().index, palette='plasma')\n",
        "plt.title('Distribution of News Categories', fontsize=16)\n",
        "plt.xlabel('Number of Articles', fontsize=12)\n",
        "plt.ylabel('Category', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nEDA visualizations have been generated.\")"
      ],
      "metadata": {
        "id": "KhCn0sURjX2o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}